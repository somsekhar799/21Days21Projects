{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NJx0h9t6HCf-YbsBe5Rg_d7kEYEnPIzR","timestamp":1758875290868}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Assignment: Face Resolution Enhancement\n","\n","**Goal:** The objective of this assignment is to implement a solution that takes a low-resolution face image (64x64 pixels) as input and produces a high-resolution, enhanced face image (256x256 pixels) using a UNET model.\n","\n","This assignment is based on the concepts and techniques demonstrated in the following Kaggle notebook:\n","\n","[https://www.kaggle.com/code/ashishjangra27/face-resolution-enhancement-with-unet](https://www.kaggle.com/code/ashishjangra27/face-resolution-enhancement-with-unet)\n","\n","You will need to refer to this resource to understand how to:\n","\n","1.  Load or define the UNET model architecture.\n","2.  Load and preprocess the input image to the required 64x64 size.\n","3.  Apply the UNET model to the input image.\n","4.  Handle the output to obtain the 256x256 enhanced image.\n","5.  (Optional but recommended) Display both the original 64x64 image and the resulting 256x256 image for comparison.\n","\n","This assignment focuses on applying the knowledge gained from the provided resource to a specific image enhancement task."],"metadata":{"id":"dtM0_eVaAJzg"}},{"cell_type":"markdown","source":["# Image Super-Resolution using U-Net\n","\n","This notebook demonstrates how to build and train a U-Net model for image super-resolution.  \n","The goal is to take a low-resolution image (64x64) and generate a high-resolution version (256x256).\n","\n","The U-Net architecture is well-suited for this task as it effectively captures both local and global features through its encoder-decoder structure with skip connections."],"metadata":{"id":"OdwtIK83Arej"}},{"cell_type":"markdown","source":["### 1. Setup and Imports\n","\n","This cell imports all the necessary libraries for building and training the U-Net model, including Keras for model definition, OpenCV for image processing, and Matplotlib for visualization."],"metadata":{"id":"o7WQWL3HAvXw"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"otR0UckWABCP","executionInfo":{"status":"ok","timestamp":1758875391100,"user_tz":-330,"elapsed":6418,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}}},"outputs":[],"source":["import os\n","import cv2 as cv\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from keras.models import Model, load_model\n","from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n","from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, GlobalMaxPool2D, concatenate\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","source":["### 2. U-Net Model Definition\n","\n","This cell defines the U-Net model architecture for image super-resolution. The `unet_64to256` function creates a U-Net model that takes a 64x64x3 image as input and outputs a 256x256x3 image. It includes an encoder, a bottleneck, and a decoder with skip connections."],"metadata":{"id":"xY-wfOl6BO5Z"}},{"cell_type":"code","source":["def unet_64to256(input_shape=(64, 64, 3), n_classes=3, final_activation='sigmoid', dropout_rate=0.05):\n","    inputs = Input(shape=input_shape, name='img')\n","\n","    # Encoder\n","    c1 = Conv2D(16, (3,3), padding='same')(inputs)\n","    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n","    c1 = Conv2D(16, (3,3), padding='same')(c1)\n","    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n","    p1 = MaxPooling2D((2,2))(c1); p1 = Dropout(dropout_rate)(p1)   # 64 -> 32\n","\n","    c2 = Conv2D(32, (3,3), padding='same')(p1)\n","    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n","    c2 = Conv2D(32, (3,3), padding='same')(c2)\n","    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n","    p2 = MaxPooling2D((2,2))(c2); p2 = Dropout(dropout_rate)(p2)   # 32 -> 16\n","\n","    c3 = Conv2D(64, (3,3), padding='same')(p2)\n","    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n","    c3 = Conv2D(64, (3,3), padding='same')(c3)\n","    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n","    p3 = MaxPooling2D((2,2))(c3); p3 = Dropout(dropout_rate)(p3)   # 16 -> 8\n","\n","    c4 = Conv2D(256, (3,3), padding='same')(p3)\n","    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n","    c4 = Conv2D(256, (3,3), padding='same')(c4)\n","    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n","    p4 = MaxPooling2D((2,2))(c4); p4 = Dropout(dropout_rate)(p4)   # 8 -> 4\n","\n","    # Bottleneck (4x4)\n","    c5 = Conv2D(256, (3,3), padding='same')(p4)\n","    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n","    c5 = Conv2D(256, (3,3), padding='same')(c5)\n","    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n","\n","    # Decoder (back to 64x64)\n","    u6 = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same')(c5)  # 4 -> 8\n","    u6 = concatenate([u6, c4]); u6 = Dropout(dropout_rate)(u6)\n","    u6 = Conv2D(256, (3,3), padding='same')(u6)\n","    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n","    u6 = Conv2D(256, (3,3), padding='same')(u6)\n","    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n","\n","    u7 = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(u6)    # 8 -> 16\n","    u7 = concatenate([u7, c3]); u7 = Dropout(dropout_rate)(u7)\n","    u7 = Conv2D(64, (3,3), padding='same')(u7)\n","    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n","    u7 = Conv2D(64, (3,3), padding='same')(u7)\n","    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n","\n","    u8 = Conv2DTranspose(32, (3,3), strides=(2,2), padding='same')(u7)    # 16 -> 32\n","    u8 = concatenate([u8, c2]); u8 = Dropout(dropout_rate)(u8)\n","    u8 = Conv2D(32, (3,3), padding='same')(u8)\n","    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n","    u8 = Conv2D(32, (3,3), padding='same')(u8)\n","    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n","\n","    u9 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u8)    # 32 -> 64\n","    u9 = concatenate([u9, c1]); u9 = Dropout(dropout_rate)(u9)\n","    u9 = Conv2D(16, (3,3), padding='same')(u9)\n","    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n","    u9 = Conv2D(16, (3,3), padding='same')(u9)\n","    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n","\n","    u10 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u9)    # 64 -> 128\n","    #u10 = concatenate([u10, c1]); u10 = Dropout(dropout_rate)(u10)\n","    u10 = Conv2D(16, (3,3), padding='same')(u10)\n","    u10 = BatchNormalization()(u10); u9 = Activation('relu')(u10)\n","    u10 = Conv2D(16, (3,3), padding='same')(u10)\n","    u10 = BatchNormalization()(u10); u9 = Activation('relu')(u10)\n","\n","    # Extra upsample to reach 256x256 (no skip)\n","    u11 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u10)   # 128 -> 256\n","    u11 = Dropout(dropout_rate)(u11)\n","    u11 = Conv2D(16, (3,3), padding='same')(u11)\n","    u11 = BatchNormalization()(u11); u10 = Activation('relu')(u11)\n","\n","    outputs = Conv2D(n_classes, (1,1), activation=final_activation, name='mask')(u11)\n","\n","    return Model(inputs=inputs, outputs=outputs, name='UNet_64to256')"],"metadata":{"id":"6KQYsnT3A1L8","executionInfo":{"status":"ok","timestamp":1758875392379,"user_tz":-330,"elapsed":3,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 3. Model Initialization and Compilation\n","\n","This cell initializes the U-Net model with the specified input shape and number of classes. It then compiles the model using the Adam optimizer and mean absolute error (MAE) as the loss function. The input and output shapes are printed to verify the model architecture."],"metadata":{"id":"aoH4yF8xBwrg"}},{"cell_type":"code","source":["model = unet_64to256(input_shape=(64,64,3), n_classes=3, final_activation='sigmoid')\n","\n","print('Input shape:', model.input_shape)   # (None, 64, 64, 3)\n","print('Output shape:', model.output_shape) # (None, 256, 256, 3)\n","\n","model.compile(optimizer=Adam(1e-4), loss='mae' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-AyI5v-BuZu","executionInfo":{"status":"ok","timestamp":1758875397189,"user_tz":-330,"elapsed":1783,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}},"outputId":"234fb5e7-28bc-4779-e486-3c9e85e7aff4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: (None, 64, 64, 3)\n","Output shape: (None, 256, 256, 3)\n"]}]},{"cell_type":"markdown","source":["### 4. Data Generator\n","\n","This cell defines a data generator function `datagen` that loads and preprocesses images from the CelebA dataset. It resizes the images to 256x256 as ground truth and to 64x64 as low-resolution input, and normalizes the pixel values. The generator yields batches of low-resolution and high-resolution image pairs for training."],"metadata":{"id":"WzimWBAEBzMo"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hnu19hrGnSY","executionInfo":{"status":"ok","timestamp":1758875405440,"user_tz":-330,"elapsed":4638,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}},"outputId":"af3c002e-b57a-4450-c8ef-5d8a72024022"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'celeba-dataset' dataset.\n","Path to dataset files: /kaggle/input/celeba-dataset\n"]}]},{"cell_type":"code","source":["import os\n","import cv2 as cv\n","import numpy as np\n","\n","# Original Kaggle path\n","imgs = os.listdir('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/')\n","\n","\n","def datagen(batch_size):\n","\n","    while True:\n","        x_batch = []\n","        y_batch = []\n","\n","        for _ in range(batch_size):\n","            indx = np.random.randint(0, len(imgs))\n","\n","            bgr = cv.imread('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' + imgs[indx])\n","            bgr = cv.resize(bgr, (256, 256))\n","            rgb = cv.cvtColor(bgr, cv.COLOR_BGR2RGB)\n","\n","            # blur = cv.blur(rgb, (4, 4)) # This line is commented out in original code\n","\n","            x = cv.resize(rgb, (64, 64))\n","            x = x / 255.0\n","            y = rgb / 255.0\n","\n","            x_batch.append(x)\n","            y_batch.append(y)\n","\n","        x_batch = np.array(x_batch).reshape(batch_size, 64, 64, 3)\n","        y_batch = np.array(y_batch).reshape(batch_size, 256, 256, 3)\n","\n","        yield x_batch, y_batch"],"metadata":{"id":"IqKyceCVB01m","executionInfo":{"status":"ok","timestamp":1758875407959,"user_tz":-330,"elapsed":1792,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 5. Model Training (Short Run)\n","\n","This cell trains the U-Net model for a small number of epochs (5) using the `datagen` function. This is a short run to quickly check if the model is training without errors."],"metadata":{"id":"KsPA0x1iB2rI"}},{"cell_type":"code","source":["batch_size = 32\n","\n","batch_size = 32\n","num_images = len(imgs)\n","steps_per_epoch = num_images // batch_size\n","\n","\n","results = model.fit(datagen(batch_size=batch_size), steps_per_epoch=100 , epochs=5, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdxBc4NNB4lQ","executionInfo":{"status":"ok","timestamp":1758875603352,"user_tz":-330,"elapsed":169334,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}},"outputId":"67c7da10-fb8c-4e01-8283-2dee3999f27e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 263ms/step - loss: 0.2469\n","Epoch 2/5\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 261ms/step - loss: 0.0857\n","Epoch 3/5\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 249ms/step - loss: 0.0668\n","Epoch 4/5\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 247ms/step - loss: 0.0578\n","Epoch 5/5\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 244ms/step - loss: 0.0533\n"]}]},{"cell_type":"markdown","source":["### 6. Model Training with Callbacks and Sample Saving\n","\n","This cell trains the U-Net model for a longer duration (3 epochs) and includes custom callbacks. The `show_and_save_samples` function generates and saves sample super-resolved images at the end of each epoch, along with the original, ground truth, and low-resolution images for comparison. The `LambdaCallback` is used to execute this function after each epoch. The model checkpoints are also saved during training."],"metadata":{"id":"vkcyQZ5bB6O1"}},{"cell_type":"code","source":["import os, cv2 as cv, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n","\n","def show_and_save_samples(model, imgs, base_dir, epoch, n=5, save_dir='./epoch_samples'):\n","    os.makedirs(save_dir, exist_ok=True)\n","    chosen = np.random.choice(imgs, n, replace=False)\n","    fig, axes = plt.subplots(n, 4, figsize=(16, 4*n))\n","    if n == 1: axes = np.expand_dims(axes, 0)\n","\n","    for i, fname in enumerate(chosen):\n","        path = os.path.join(base_dir, fname)\n","        rgb = cv.cvtColor(cv.imread(path), cv.COLOR_BGR2RGB)\n","        gt256 = cv.resize(rgb, (256,256)).astype('float32')/255\n","        img64 = cv.resize(rgb, (64,64)).astype('float32')/255\n","        pred  = model.predict(np.expand_dims(img64,0), verbose=0)[0]\n","        lowup = cv.resize((img64*255).astype(np.uint8),(256,256))\n","\n","        for ax, im, title in zip(axes[i],[rgb,gt256,lowup,pred],\n","                                 ['Original','GT 256x256','Low-Res','Prediction']):\n","            ax.imshow(im); ax.set_title(title); ax.axis('off')\n","\n","    plt.tight_layout(); plt.savefig(f\"{save_dir}/epoch_{epoch+1:02d}.png\"); plt.show()\n","\n","# --- Simple Callbacks ---\n","def on_epoch_end(epoch, logs):\n","    show_and_save_samples(model, imgs, base_dir, epoch, n=5)\n","    model.save(f'./checkpoints/model_epoch_{epoch+1:02d}.keras')\n","\n","show_cb = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n","\n","\n","batch_size = 32\n","steps_per_epoch = len(imgs)//batch_size\n","base_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","os.makedirs('./checkpoints', exist_ok=True)\n","\n","results = model.fit(\n","    datagen(batch_size=batch_size),\n","    steps_per_epoch=100,\n","    epochs=3,\n","    callbacks=[show_cb],\n","    verbose=1)"],"metadata":{"id":"kkbCSFmZB8KM","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gE-6orS1W8sFHyRWDVL--01EO6ZIcbC-"},"executionInfo":{"status":"ok","timestamp":1758875740371,"user_tz":-330,"elapsed":93886,"user":{"displayName":"Kamisetti Som Sekhar","userId":"06406103987280812483"}},"outputId":"efe91bc9-f80d-478c-ec8d-77ac4d2bcf68"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["### 7. Project Summary\n","\n","This notebook successfully implemented and trained a U-Net model for image super-resolution. The model was trained on the CelebA dataset, taking 64x64 images as input and generating 256x256 images.\n","\n","The training process showed a decrease in the mean absolute error (MAE) loss over epochs, indicating that the model is learning to reconstruct higher-resolution images. The generated sample images at the end of each epoch provide a visual representation of the model's progress.\n","\n","Further improvements could include:\n","- Experimenting with different loss functions (e.g., perceptual loss)\n","- Using a larger and more diverse dataset\n","- Implementing more advanced super-resolution techniques (e.g., Generative Adversarial Networks)\n","- Hyperparameter tuning to optimize model performance"],"metadata":{"id":"JjlGDecpB9tr"}},{"cell_type":"code","source":[],"metadata":{"id":"C17ThOoSKiDo"},"execution_count":null,"outputs":[]}]}